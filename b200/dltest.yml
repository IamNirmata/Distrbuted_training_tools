apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  generateName: gcr-daily-validation-hari-
  namespace: gcr-admin
spec:
  queue: gcr-admin
  minAvailable: 1 # for gang scheduling requires minAvailable to be set to the total number of tasks
  plugins:
    ssh: []   # passwordless SSH + host files under /etc/volcano
    svc: []   # creates per-task headless Services when containerPorts are present
    env: []   # VC_* envs
  tasks:
    - name: server
      replicas: 1
      template:
        metadata:
          labels:
            app: allpair-mpi
            role: server
        spec:
          schedulerName: volcano
          restartPolicy: Never
          volumes:
            - name: dshm # use /dev/shm for inter-process communication
              emptyDir:
                medium: Memory
                sizeLimit: 100Gi
            - name: data
              persistentVolumeClaim:
                claimName: pvc-local-gcr-admin-01
          tolerations:
            - key: "rdma"
              operator: "Exists"
              effect: "NoSchedule"
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchLabels:
                      app: allpair-mpi
                  topologyKey: kubernetes.io/hostname
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: kubernetes.io/hostname
                        operator: In
                        values:
                          - gcr-node-name-placeholder # this will be replaced by the node name using jobrunner.sh script
          containers:
            - name: server
              image: nvcr.io/nvidia/pytorch:25.06-py3
              env:
                - name: gcrnode
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
              volumeMounts:
                - name: dshm
                  mountPath: /dev/shm
              ports:
                - name: rdma
                  containerPort: 18515
              command: ["/bin/bash","-lc"]
              args:
                - |
                  #1
                  echo "Starting server..."
                  #distrbuted_training_tools
                  git clone https://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@github.com/IamNirmata/distrbuted_training_tools.git /opt/distrbuted_training_tools
                  cd /opt/distrbuted_training_tools/b200/
                  bash envset.sh
                  #2
                  #setup pvc
                  bash setup_pvc.sh
                  #3
                  #deeplearning_unit_test
                  git clone https://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@github.com/IamNirmata/deeplearning_unit_test.git /opt/deeplearning_unit_test
                  cd /opt/deeplearning_unit_test
                  bash run_b200.sh 8
                  #3
                  sleep 6000
              volumeMounts:
                - name: data
                  mountPath: /data
              resources:
                requests:
                  nvidia.com/gpu: "8"
                  cpu: "180"
                  memory: "2500Gi"
                  rdma/rdma_shared_device_a: "1"
                limits:
                  nvidia.com/gpu: "8"
                  cpu: "180"
                  memory: "2500Gi"
                  rdma/rdma_shared_device_a: "1"
 